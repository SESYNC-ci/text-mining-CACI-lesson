---
---

## Text Mining

Developing measurements of quantitative variables from unstructured information
is another component of the "field-work" in research projects that rely on texts
for empirical observations.

- Searching strings for patterns
- Cleaning strings of un-informative patterns
- Quantifying string occurrences and associations
- Interpretting the meaning of associated strings

===

## Cleaning Text

Assuming the structured data in the Enron e-mail headers has been captured,
strip down the content to the unstructured message.

```{r, handout = 0}
enron <- tm_map(enron, function(email) {
  body <- content(email)
  match <- str_detect(body, '^X-FileName:')
  begin <- which(match)[[1]] + 1
  match <- str_detect(body, '^[>\\s]*[_\\-]{2}')
  match <- c(match, TRUE)
  end <- which(match)[[1]] - 1
  content(email) <- body[begin:end]
  return(email)
})
```

```{r}
email <- enron[[2]]
content(email)
```
===

## Predefind Cleaning Functions

These are some of the functions listed by `getTransformations`.

```{r, handout = 0}
library(magrittr)

enron_words <- enron %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(stripWhitespace)
```

```{r}
email <- enron_words[[2]]
content(email)
```

===

Customize document preparation with your own functions. The function must be
wrapped in `content_transformer` if designed to accept and return strings rather
than PlainTextDocuments.

```{r, handout = 0}
remove_link <- function(body) {
  match <- str_detect(body, '(http|www|mailto)')
  body[!match]
}
enron_words <- enron_words %>%
  tm_map(content_transformer(remove_link))  
```

===

# Stopwords and Stems

Stopwords are the throwaway words that don't inform content, and lists for
different languages are complied within **tm**. Before removing them though,
also "stem" the current words to remove plurals and other nuissances.
{:.notes}

```{r, handout = 0}
enron_words <- enron_words %>%
  tm_map(stemDocument) %>%
  tm_map(removeWords, stopwords("english"))
```

===

## Document-Term Matrix

The "bag-of-words" model for turning a corpus into structured data is
to simply count the word frequency in each document.

===

```{r, handout = 0}
dtm <- DocumentTermMatrix(enron_words)
```

===

## Long Form

The [tidytext](){:.rlib} package converts the (wide) Document Term Matrix
into a longer form table with a row for every document and term combination.

===

```{r, handout = 0}
library(tidytext)
library(dplyr)

dtt <- tidy(dtm)
words <- dtt %>%
  group_by(term) %>%
  summarise(
    n = n(),
    total = sum(count)) %>%
  mutate(nchar = nchar(term))
```

===

The `words` data frame is more amenable to further inspection and
cleaning, such as removing outliers.

```{r, handout = 0}
library(ggplot2)

ggplot(words, aes(x = nchar)) +
  geom_histogram(binwidth = 1)
```

===

Words with too many characters are probably not actually words, and extremely
uncommon words won't help when searching for patterns.

```{r, handout = 0}
dtt_trimmed <- words %>%
  filter(
    nchar < 16,
    n > 1,
    total > 3) %>%
  select(term) %>%
  inner_join(dtt)
```

===

Further steps in analyzing this "bag-of-words" require returning to the
Document-Term-Matrix structure.

```{r, handout = 0}
dtm_trimmed <- dtt_trimmed %>%
  cast_dtm(document, term, count)
```

```{r}
dtm_trimmed
```

===

## Term Correlations

The `findAssocs` function checks columns of the document-term matrix for
correlations. For example, words that are associated with the name of the CEO,
Ken Lay.

```{r, handout = 0}
word_assoc <- findAssocs(dtm_trimmed, 'ken', 0.6)
word_assoc <- data.frame(
  word = names(word_assoc[[1]]),
  assoc = word_assoc,
  row.names = NULL)
```

===

A "wordcloud" emphasizes those words that have a high co-occurence
in the email corpus with the CEO's first name.

```{r}
library(ggwordcloud)

ggplot(word_assoc,
  aes(label = word, size = ken)) +
  geom_text_wordcloud_area()
```
